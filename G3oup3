# -*- coding: utf-8 -*-
"""NLPB2_Lab_Practicum_Group3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164Nz22mLbA6sZwDHa7Q9ROoto_UsSKbD
"""

!pip install transformers torch pandas

import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
import torch


formatted_output = pd.read_csv("formatted_output.csv")
formatted_output[['Question', 'Answer']] = formatted_output['Q&A'].str.extract(r'Question:\s*(.+)\s*Answer:\s*(.+)', expand=True)
formatted_output = formatted_output[['Question', 'Answer']]

from sklearn.model_selection import train_test_split


tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


class QADataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length=512):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions[idx]
        answer = self.answers[idx]


        inputs = self.tokenizer(question, answer, truncation=True, padding="max_length", max_length=self.max_length, return_tensors="pt")


        inputs = {key: val.squeeze(0) for key, val in inputs.items()}


        label = torch.tensor(1 if idx < len(self.questions) // 2 else 0, dtype=torch.long)
        inputs["labels"] = label

        return inputs


train_questions, test_questions, train_answers, test_answers = train_test_split(
    formatted_output['Question'].values,
    formatted_output['Answer'].values,
    test_size=0.2,
    random_state=42
)


train_dataset = QADataset(train_questions, train_answers, tokenizer)
test_dataset = QADataset(test_questions, test_answers, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from transformers import AdamW


optimizer = AdamW(model.parameters(), lr=2e-5)


model.train()
for batch in train_loader:
    optimizer.zero_grad()
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    print(f"Loss: {loss.item()}")
